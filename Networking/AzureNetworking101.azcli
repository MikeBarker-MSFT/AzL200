# Code source https://github.com/MikeBarker-MSFT/AzL200
# You MUST update the 'unique' in 'Variables' to ensure no conflict with other DNS names 
# Simple Azure networking lab to demonstrate a simple web app in VM scale set with Azure Load Balancer, deployed in 2 Azure Regions with an Azure Front Door to load balance across all instances

# ----------- Cloud Drive --------------
# If you are using the Azure Cloud Shell and you haven't used it before, you will need to set up your Cloud Drive.
# Defaults should be fine
# Recommend you create a new folder in Cloud Drive to store your files in
cd ~/clouddrive
mkdir networking101
cd networking101

# ------------ Variables -----------------------------
#Use export to declare variables so they can be re-used

#All Resources will be created in this Resource Group
export rg="1LbAndFrontDoor"

# Unique 4 characters - used to create public DNS names 
export unique="mawc"

# We will create resources in 2 Azure Regions. By default WestEurope and NorthEurope but you can use others
# Use this command to see a list of available Regions - use the 'Name' (lowercase, single word) not the 'Display Name'
az account list-locations --query "[].{DisplayName:displayName, Name:name}" -o table

export loc1="westeurope"
# Create a short code (locs = location short) for location above - this will be appended to resource names to make it easier to identify which you are connecting to
export locs1="weu"

export loc2="northeurope"
export locs2="neu"

# Use these variables to build the DNS names we will use
export dns1="vmss-"$unique"."$loc1".cloudapp.azure.com"
export dns2="vmss-"$unique"."$loc2".cloudapp.azure.com"
export afd=$unique"afd1"

# ----------- Test uniqueness ----------

# These must fail to resolve or else you will get deployment errors due to name conflicts
nslookup $dns1
nslookup $dns2
nslookup $afd.azurefd.net

# ----------- Subscription -------------
#Check which Azure Subscription you are connected to
az account show 

#If it's not the right one then check which ones you have access to and use 'set' to choose the right subscription Id
az account list
az account set -s $SubscriptionId

# ------------ Region 1 -------------------------

export loc=$loc1
export locs=$locs1 

# ------------ Resource Group -------------------------

#Create a Resource Group in Region 1 - all resources will be created in this same RG to make it easier to find and view them
az group create -n $rg -l $loc


# ---------- Cloud Init -----------------

# Creates cloud-init file which gets run by the Linux VM at 1st boot
# Cloud Init is very useful for configuring VMs automatically
# The VM will run Express to serve a simple web page on localhost:3000 and then use Nginx to proxy inbound connections on port 80 to that
echo "#cloud-config
package_upgrade: true
packages:
  - nginx
  - nodejs
  - npm
write_files:
  - owner: www-data:www-data
  - path: /etc/nginx/sites-available/default
    content: |
      server {
        listen 80;
        location / {
          proxy_pass http://localhost:3000;
          proxy_http_version 1.1;
          proxy_set_header Upgrade \$http_upgrade;
          proxy_set_header Connection keep-alive;
          proxy_set_header Host \$host;
          proxy_cache_bypass \$http_upgrade;
        }
      }
  - owner: azureuser:azureuser
  - path: /home/azureuser/myapp/index.js
    content: |
      var express = require('express')
      var app = express()
      var os = require('os');
      app.get('/', function (req, res) {
        res.send('Hello World from host ' + os.hostname())
      })
      app.listen(3000, function () {
        console.log('Hello world app listening on port 3000!')
      })
runcmd:
  - service nginx restart
  - cd "/home/azureuser/myapp"
  - npm init
  - npm install express -y
  - npm install pm2 -g
  - pm2 start /home/azureuser/myapp/index.js
  - env PATH=$PATH:/usr/bin /usr/local/lib/node_modules/pm2/bin/pm2 startup systemd 

" > cloud-init.txt

# ------------- MAIN -----------------------------

# Create a VNet to host the VM's being created in the next steps
az network vnet create \
    -g $rg \
    -l $loc \
    -n Vnet1-$locs \
    --subnet-name Subnet1

# Create a Network Security Group (NSG)
az network nsg create \
    -g $rg \
    -l $loc \
    -n NSG-$locs

#Create a NSG rule to allow inbound http traffic 
az network nsg rule create \
    -g $rg \
    -n Http \
    --nsg-name NSG-$locs \
    --protocol tcp \
    --direction inbound \
    --source-address-prefix '*' \
    --source-port-range '*' \
    --destination-address-prefix '*' \
    --destination-port-range 80 \
    --access allow \
    --priority 200

# Create a VM Scale Set  https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/quick-create-cli 
# You can open the Azure portal and look in the Resource Group you created to see the resources getting created. Click 'Deployments' to see the steps taken
# Using --verbose with this one so we can see in console exactly what it is doing
az vmss create \
    -g $rg \
    -n $locs \
    -l $loc \
    --image UbuntuLTS \
    --instance-count 3 \
    --lb-sku Standard \
    --vnet-name Vnet1-$locs \
    --subnet Subnet1 \
    --custom-data cloud-init.txt \
    --vm-sku Standard_B1s \
    --nsg NSG-$locs \
    --public-ip-address-dns-name "vmss-"$unique \
    --generate-ssh-keys \
    --verbose

# Add an HTTP probe to the load balancer so it checks the backend health
az network lb probe create \
    -g $rg \
    -n HttpProbe \
    --lb-name $locs"LB" \
    --protocol http \
    --port 80 \
    --path / \
    --interval 5 \
    --threshold 2

# Update Load Balancing rule to use the new health probe 
az network lb rule update \
    -g $rg \
    -n LBRule \
    --lb-name $locs"LB" \
    --probe-name HttpProbe


# Check the progress of deployment
az vmss list-instances -g $rg -n $locs -o table

# ---------------- Test Load Balancing in Region 1 -----------------

# Even after instances are all deployed it will take a minute or 2 before they start accepting connections - this is while the cloud-init builds
# If you didn't add a health probe, you will see "connection refused" at first and then "Bad Gateway" for short while and then it should start working
# If you did add the health probe it will just return "connection timed out" until it starts working 

# Use this 'For' loop to test connecting to the public IP 20 times
# You should be load balanced between the VMs
for i in {1..20}; do curl $dns1 -m 1; echo -n $'\r\f'; sleep 0.1; done



# ----------------- Region 2 ------------------------------------

# Now we change the location variables to the 2nd Region and run MAIN again to create an identical setup in another region.
# The power of Infrastructure as Code!

export loc=$loc2
export locs=$locs2

# ---------------- Test Load Balancing in Region 2 -----------------

for i in {1..20}; do curl $dns2 -m 1; echo -n $'\r\f'; sleep 0.1; done


# ----------------- Front Door ---------------------------------

# The latest Azure Front Door extensions may need to be added to your Cloud Shell instance
az extension add --name front-door

# Create AFD
az network front-door create -g $rg -n $afd --backend-address $dns1 --interval 5 --protocol http 

# Update Load Balancing rule for AFD to fail-over quicker and set the latency threshold to high enough so it will use both back-ends
az network front-door load-balancing create -g $rg -f $afd -n DefaultLoadBalancingSettings --sample-size 1 --successful-samples-required 1 --additional-latency 2000

# Add the 2nd Region to the back-end pool 
az network front-door backend-pool backend add -g $rg -f $afd --pool-name DefaultBackendPool --address $dns2

# ----------------- Test Front Door -----------------------------

# It may take a minute or 2 for the health checks in both regions to pass and for AFD to start sending them traffic 
for i in {1..20}; do curl $afd.azurefd.net -m 1; echo -n $'\r\f'; sleep 0.1; done


# ----------------- Additional Tasks ----------------------------------

# 1 - Try opening Network Watcher in the portal and viewing the Topolgy of your Resource Group to see the components you've deployed (doesn't show AFD yet)
# 2 - Try scaling your VMSS out or in and see Azure adjust the load-balancing automatically
# 3 - Try disabling one of the Backends in AFD or lower the acceptable 'latency sensitivity' to zero so it only sends traffic to the closest backend
# 4 - Investigate Load Balancer metrics to see 'Data Path Availability' and 'Health Probe Status' to see how LB decides which backends to send traffic to. Optionally split Health Probe Status by Backend IP address
# 5 - Add AFD metrics: Backend Health Percentage and Backend Request Latency and split by Backend 
